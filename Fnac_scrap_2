import time
import random
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import csv
from datetime import datetime
from bs4 import BeautifulSoup

# Set up the Selenium WebDriver
options = webdriver.ChromeOptions()
# Add stealth options
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-gpu')
options.add_argument('--log-level=3')  # Only show fatal errors
options.add_experimental_option('excludeSwitches', ['enable-logging'])  # Suppress console logging
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--disable-notifications')
options.add_argument('--start-maximized')
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)
options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def get_page_content():
    time.sleep(random.uniform(3, 6))
    
    # Ensure page loads fully before scraping
    WebDriverWait(driver, 15).until(
        EC.presence_of_element_located((By.CLASS_NAME, 'Article-mosaicItem'))
    )
    
    # Scroll down to trigger any lazy loading
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(5)
    
    # Check if products exist in page source
    page_source = driver.page_source
    if 'Article-mosaicItem' in page_source:
        return page_source
    
    print("No products found in page source")
    return None

def extract_product_details(content):
    soup = BeautifulSoup(content, 'html.parser')
    print("Parsing page content...")
    
    products = soup.find_all('div', class_='Article-mosaicItem')
    print(f"Found {len(products)} products")
    
    product_details = []
    
    for product in products:
        try:
            # Extract URL and title
            url_tag = product.find('a', class_='thumbnail-titleLink')
            url = 'https://www.fnac.com' + url_tag['href'] if url_tag and 'href' in url_tag.attrs else ''
            title = url_tag.text.strip() if url_tag else ''
            
            # Extract price
            price_tag = product.find('span', class_='thumbnail-price')
            price = price_tag.text.strip() if price_tag else ''
            
            # Extract image URL
            img_tag = product.find('img', class_='thumbnail-imgContent')
            img_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else ''
            
            current_date = datetime.now().strftime("%d/%m/%Y")
            
            if title and price:
                product_details.append([current_date, title, price, url, img_url])
                print(f"Found product: {title[:30]}... - {price}")
            
        except Exception as e:
            print(f"Error processing product: {str(e)}")
    
    return product_details

def close_cookie_popup():
    """Close the cookie consent popup if it appears."""
    try:
        cookie_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))  # Adjust selector if needed
        )
        cookie_button.click()
        print("Closed cookie popup")
        time.sleep(2)  # Short delay after closing
    except Exception as e:
        print("No cookie popup found or already closed.")


def click_next_page():
    """Click the 'Voir plus d'articles' button and wait for new products to load."""
    try:
        close_cookie_popup()  # Make sure the popup isn't blocking

        next_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CLASS_NAME, "js-InfiniteScrollNextBtn"))
        )
        driver.execute_script("arguments[0].scrollIntoView();", next_button)  # Scroll to button
        time.sleep(2)  # Small delay
        next_button.click()
        print("Clicked 'Voir plus d'articles' button")
        time.sleep(5)  # Allow new products to load
        return True
    except Exception as e:
        print("Failed to click 'Voir plus d'articles':", e)
        return False


try:
    # Base URL for page 1
    base_url = "https://www.fnac.com/Tous-les-Videoprojecteurs/Videoprojecteur/nsh474921/w-4?PageIndex=1&SDM=mosaic&SFilt=1!13"
    all_product_details = []

    print("\nScraping page 1")
    driver.get(base_url)
    page_content = get_page_content()
    
    page_products = extract_product_details(page_content)
    
    if page_products:
        all_product_details.extend(page_products)
        print(f"Total products found so far: {len(all_product_details)}")
    else:
        print("No products found on page 1")

    # Try clicking "Next Page" instead of manually navigating
    if click_next_page():
        print("\nScraping page 2")
        time.sleep(5)  # Allow page to load
        page_content = get_page_content()
        
        page_products = extract_product_details(page_content)
        
        if page_products:
            all_product_details.extend(page_products)
            print(f"Total products found so far: {len(all_product_details)}")
        else:
            print("No products found on page 2")
    
    # Write results to CSV
    with open('product_details.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Date', 'Title', 'Price', 'URL', 'Image URL'])
        csvwriter.writerows(all_product_details)

    print(f"\nScraping completed. Found {len(all_product_details)} products.")
    print("Output saved in 'product_details.csv'.")

except Exception as e:
    print(f"An error occurred: {str(e)}")
    # Save collected data before failure
    if all_product_details:
        with open('product_details.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:
            csvwriter = csv.writer(csvfile)
            csvwriter.writerow(['Date', 'Title', 'Price', 'URL', 'Image URL'])
            csvwriter.writerows(all_product_details)
        print(f"Saved {len(all_product_details)} products before error occurred.")

finally:
    driver.quit()
